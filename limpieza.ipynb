{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5300f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Angelica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Original (primeras 5 filas) ---\n",
      "               film_name   gender  film_avg_rate  review_rate  \\\n",
      "0  Ocho apellidos vascos  Comedia            6.0          3.0   \n",
      "1  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "2  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "3  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "4  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "\n",
      "                                        review_title  \\\n",
      "0     OCHO APELLIDOS VASCOS...Y NING√öN NOMBRE PROPIO   \n",
      "1                                     El perro verde   \n",
      "2  Si no eres de comer mierda... no te comas esta...   \n",
      "3                                    Aida: The movie   \n",
      "4               UN HOMBRE SOLO (Julio Iglesias 1987)   \n",
      "\n",
      "                                         review_text    genre  \n",
      "0  La mayor virtud de esta pel√≠cula es su existen...  comedia  \n",
      "1  No soy un experto cin√©filo, pero pocas veces m...  comedia  \n",
      "2  Si no eres un incondicional del humor estilo T...  comedia  \n",
      "3  No s√© qu√© est√° pasando, si la gente se deja ll...  comedia  \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...  comedia  \n",
      "\n",
      "N√∫mero total de comentarios cargados: 8603\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- INICIANDO LIMPIEZA DE DATOS ---\n",
      "\n",
      "--- Comentarios despu√©s de limpieza general (primeras 5 filas) ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta pel√≠cula es su existen...   \n",
      "1  No soy un experto cin√©filo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No s√© qu√© est√° pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                              cleaned_review_general  \n",
      "0  la mayor virtud de esta pelicula es su existen...  \n",
      "1  no soy un experto cinefilo pero pocas veces me...  \n",
      "2  si no eres un incondicional del humor estilo t...  \n",
      "3  no se que esta pasando si la gente se deja lle...  \n",
      "4  pero cuando amanecey me quedo solosiento en el...  \n",
      "\n",
      "Procesando comentarios con SpaCy para lematizaci√≥n y eliminaci√≥n de stopwords...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "# from spacy import displacy # Descomenta si quieres usarlo para visualizar √°rboles\n",
    "\n",
    "# --- 0. Configuraci√≥n Inicial y Descarga de Recursos ---\n",
    "# Descargar recursos de NLTK (solo la primera vez que se ejecute en un entorno nuevo)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Descargar el modelo de espa√±ol de SpaCy (solo la primera vez)\n",
    "# Aseg√∫rate de haber instalado spacy previamente: pip install spacy\n",
    "# y luego, en tu terminal: python -m spacy download es_core_news_sm\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Descargando modelo de espa√±ol de SpaCy (es_core_news_sm)...\")\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# --- 1. Cargar tu Dataset ---\n",
    "# --- ¬°¬°¬° CONFIGURA ESTAS DOS L√çNEAS SEG√öN TU ARCHIVO !!! ---\n",
    "file_path = \"data/reviews_filmaffinity_limpio.csv\" # <--- ¬°CAMBIA ESTO A LA RUTA Y NOMBRE DE TU ARCHIVO!\n",
    "file_type = \"csv\" # <--- ¬°CAMBIA ESTO A \"csv\" SI TU ARCHIVO ES CSV!\n",
    "\n",
    "if file_type == \"csv\":\n",
    "    df = pd.read_csv(file_path)\n",
    "elif file_type == \"excel\":\n",
    "    df = pd.read_excel(file_path)\n",
    "else:\n",
    "    raise ValueError(\"Tipo de archivo no soportado. Usa 'csv' o 'excel'.\")\n",
    "\n",
    "# --- ¬°¬°¬° VERIFICA ESTOS NOMBRES DE COLUMNA EN TU DATASET !!! ---\n",
    "# Ajusta si tus columnas tienen nombres diferentes en tu archivo\n",
    "GENRE_COLUMN = 'gender'\n",
    "REVIEW_TEXT_COLUMN = 'review_text'\n",
    "\n",
    "# Renombrar columnas para consistencia si no tienen los nombres esperados\n",
    "if GENRE_COLUMN not in df.columns:\n",
    "    print(f\"ERROR: La columna de g√©nero '{GENRE_COLUMN}' no se encontr√≥. Por favor, ajusta 'GENRE_COLUMN' o renombra tu columna en el archivo.\")\n",
    "    exit()\n",
    "if REVIEW_TEXT_COLUMN not in df.columns:\n",
    "    print(f\"ERROR: La columna de texto de rese√±a '{REVIEW_TEXT_COLUMN}' no se encontr√≥. Por favor, ajusta 'REVIEW_TEXT_COLUMN' o renombra tu columna en el archivo.\")\n",
    "    exit()\n",
    "\n",
    "# Asegurarse de que los g√©neros sean limpios para la clasificaci√≥n\n",
    "# Por ejemplo, de 'Ocho apellido comedia' -> 'comedia'\n",
    "# Esto asume que el g√©nero relevante es la √∫ltima palabra de la cadena en 'film_genre'\n",
    "df['genre'] = df[GENRE_COLUMN].apply(lambda x: str(x).split()[-1].lower()) # Asegura que x sea string\n",
    "\n",
    "print(\"--- DataFrame Original (primeras 5 filas) ---\")\n",
    "print(df.head())\n",
    "print(f\"\\nN√∫mero total de comentarios cargados: {len(df)}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- PARTE 2: LIMPIEZA DE DATOS (PREPROCESAMIENTO) ---\n",
    "print(\"--- INICIANDO LIMPIEZA DE DATOS ---\")\n",
    "\n",
    "# 2.1 Funci√≥n de Preprocesamiento Integral\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words_spanish = set(stopwords.words('spanish'))\n",
    "\n",
    "def clean_text_general(text):\n",
    "    \"\"\"\n",
    "    Realiza una limpieza general del texto:\n",
    "    - Convierte a min√∫sculas.\n",
    "    - Decodifica caracteres especiales (tildes, e√±es mal codificadas).\n",
    "    - Elimina URLs, puntuaci√≥n y n√∫meros.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): # Asegura que la entrada sea string\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower() # Convierte a min√∫sculas\n",
    "\n",
    "    # Decodificaci√≥n de caracteres comunes en espa√±ol que pueden venir mal codificados\n",
    "    text = text.replace('√É¬°', '√°').replace('√É¬©', '√©').replace('√É¬≠', '√≠').replace('√É¬≥', '√≥').replace('√É¬∫', '√∫')\n",
    "    text = text.replace('√É¬±', '√±').replace('√¢‚Ç¨‚Ñ¢', \"'\").replace('√¢‚Ç¨≈ì', '\"').replace('√¢‚Ç¨ ', '\"')\n",
    "    text = text.replace('√°', 'a').replace('√©', 'e').replace('√≠', 'i').replace('√≥', 'o').replace('√∫', 'u') # Normalizaci√≥n sin tildes para evitar problemas con algunos tokenizadores o modelos que no las manejen bien\n",
    "    text = text.replace('√±', 'n')\n",
    "\n",
    "\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Eliminar puntuaci√≥n y caracteres especiales (solo deja letras y espacios)\n",
    "    text = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±\\s]', '', text) # Deja letras (incluyendo acentuadas y √±) y espacios\n",
    "\n",
    "    # Eliminar n√∫meros (ya los elimina la regex anterior, pero por si acaso)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Eliminar m√∫ltiples espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    \"\"\"\n",
    "    Procesa texto con SpaCy para obtener lemas limpios (sin stopwords y no alfanum√©ricos).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    # is_alpha: filtra puntuaci√≥n y n√∫meros\n",
    "    # is_stop: filtra stopwords\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Aplicar la limpieza general a la columna de comentarios\n",
    "df['cleaned_review_general'] = df[REVIEW_TEXT_COLUMN].apply(clean_text_general)\n",
    "print(\"\\n--- Comentarios despu√©s de limpieza general (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'cleaned_review_general']].head())\n",
    "\n",
    "\n",
    "# Aplicar SpaCy para lematizaci√≥n y eliminaci√≥n de stopwords\n",
    "# Usaremos nlp.pipe para mayor eficiencia si el dataset es grande\n",
    "print(\"\\nProcesando comentarios con SpaCy para lematizaci√≥n y eliminaci√≥n de stopwords...\")\n",
    "# Esto es m√°s eficiente que .apply(lambda x: spacy_preprocess(x)) para datasets grandes\n",
    "docs = nlp.pipe(df['cleaned_review_general'].astype(str), batch_size=500, n_process=-1) # n_process=-1 usa todos los n√∫cleos disponibles\n",
    "df['spacy_doc'] = list(docs) # Almacena los objetos Doc de SpaCy\n",
    "\n",
    "# Extraer lemas limpios de los objetos Doc\n",
    "df['cleaned_review_spacy_lemmas'] = df['spacy_doc'].apply(\n",
    "    lambda doc: ' '.join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    ")\n",
    "\n",
    "print(\"\\n--- Comentarios despu√©s de SpaCy (lemas limpios) (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'cleaned_review_spacy_lemmas']].head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f637fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARTE 3: ESTAD√çSTICA A PARTIR DE LOS COMENTARIOS ---\n",
    "print(\"--- INICIANDO AN√ÅLISIS ESTAD√çSTICO ---\")\n",
    "\n",
    "# --- 3.1. Conteo y Estad√≠sticas B√°sicas ---\n",
    "\n",
    "# N√∫mero de palabras por comentario (despu√©s de limpieza)\n",
    "df['num_palabras_limpias'] = df['cleaned_review_spacy_lemmas'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# N√∫mero de oraciones por comentario (antes de la limpieza profunda, para mantener la estructura original)\n",
    "df['num_oraciones'] = df[REVIEW_TEXT_COLUMN].apply(\n",
    "    lambda x: len(nltk.sent_tokenize(str(x), language='spanish')) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Estad√≠sticas de longitud de comentarios (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'num_palabras_limpias', 'num_oraciones']].head())\n",
    "\n",
    "# Estad√≠sticas descriptivas de longitud por g√©nero\n",
    "print(\"\\n--- Estad√≠sticas descriptivas de N√∫mero de Palabras Limpias por G√©nero ---\")\n",
    "print(df.groupby('genre')['num_palabras_limpias'].describe())\n",
    "\n",
    "print(\"\\n--- Estad√≠sticas descriptivas de N√∫mero de Oraciones por G√©nero ---\")\n",
    "print(df.groupby('genre')['num_oraciones'].describe())\n",
    "\n",
    "\n",
    "# --- 3.2. Frecuencia de Palabras y N-gramas por G√©nero ---\n",
    "\n",
    "def get_top_n_items(corpus, n_items=None, item_type='word', n_gram_size=1):\n",
    "    \"\"\"\n",
    "    Obtiene los N √≠tems m√°s frecuentes (palabras o n-gramas) de un corpus.\n",
    "    item_type: 'word' para palabras, 'ngram' para n-gramas.\n",
    "    n_gram_size: tama√±o del n-grama si item_type es 'ngram'.\n",
    "    \"\"\"\n",
    "    vec = Counter()\n",
    "    for review in corpus:\n",
    "        tokens = str(review).split() # Asegura que review sea string\n",
    "        if item_type == 'word':\n",
    "            for word in tokens:\n",
    "                vec[word] += 1\n",
    "        elif item_type == 'ngram':\n",
    "            if len(tokens) >= n_gram_size: # Asegura que hay suficientes tokens para el n-grama\n",
    "                for ngram_item in ngrams(tokens, n_gram_size):\n",
    "                    vec[' '.join(ngram_item)] += 1\n",
    "    return vec.most_common(n_items)\n",
    "\n",
    "print(\"\\n--- Frecuencia de Palabras y N-gramas por G√©nero ---\")\n",
    "all_genres = df['genre'].unique()\n",
    "\n",
    "for genre_val in all_genres:\n",
    "    genre_corpus = df[df['genre'] == genre_val]['cleaned_review_spacy_lemmas']\n",
    "\n",
    "    print(f\"\\n***** An√°lisis para el g√©nero: '{genre_val.upper()}' *****\")\n",
    "\n",
    "    # Palabras m√°s frecuentes\n",
    "    top_words = get_top_n_items(genre_corpus, n_items=10, item_type='word')\n",
    "    print(f\"  Top 10 palabras (lemas): {top_words}\")\n",
    "\n",
    "    # Bigramas m√°s frecuentes\n",
    "    top_bigrams = get_top_n_items(genre_corpus, n_items=5, item_type='ngram', n_gram_size=2)\n",
    "    print(f\"  Top 5 bigramas (lemas): {top_bigrams}\")\n",
    "\n",
    "    # Trigramas m√°s frecuentes\n",
    "    top_trigrams = get_top_n_items(genre_corpus, n_items=3, item_type='ngram', n_gram_size=3)\n",
    "    print(f\"  Top 3 trigramas (lemas): {top_trigrams}\")\n",
    "\n",
    "\n",
    "# --- 3.3. An√°lisis con Expresiones Regulares ---\n",
    "\n",
    "# Define patrones espec√≠ficos para cada g√©nero. Estos son solo ejemplos, puedes expandirlos.\n",
    "genre_patterns = {\n",
    "    'comedia': [\n",
    "        r'pelicula\\s+(?:muy\\s+)?(?:divertida|graciosa|entretenida|absurda)', # Usamos ?: para grupos no capturadores\n",
    "        r'(?:reir|carcajad|humor\\s+(?:excelente|buen|genial))'\n",
    "    ],\n",
    "    'drama': [\n",
    "        r'historia\\s+(?:profunda|emotiva|triste|reflexiva|real)',\n",
    "        r'(?:llorar|conmovedor|impacto\\s+emocional)'\n",
    "    ],\n",
    "    'thriller': [\n",
    "        r'(?:intenso|suspenso|giro\\s+inesperado|misterio|intriga)'\n",
    "    ],\n",
    "    'ciencia ficci√≥n': [ # Ajustado a 'ciencia ficcion' por la normalizaci√≥n sin tildes\n",
    "        r'(?:efecto\\s+especial|ciencia\\s+ficcion|futurista|espacio|tecnologia)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def apply_regex_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Aplica una lista de patrones regex a un texto y cuenta cu√°ntos coinciden.\n",
    "    Devuelve un contador de los g√©neros detectados.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return [(\"N/A\", 0)] # Devolver un valor por defecto si no es string\n",
    "\n",
    "    detected_genres = Counter()\n",
    "    for genre, regex_list in patterns.items():\n",
    "        for pattern in regex_list:\n",
    "            if re.search(pattern, text, re.IGNORECASE): # re.IGNORECASE para coincidir independientemente de may√∫sculas/min√∫sculas\n",
    "                detected_genres[genre] += 1\n",
    "    \n",
    "    if not detected_genres: # Si no se detect√≥ ning√∫n g√©nero, devuelve 'desconocido'\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1) # Devuelve el g√©nero m√°s detectado por esta t√©cnica\n",
    "\n",
    "df['predicted_genre_regex'] = df['cleaned_review_general'].apply(\n",
    "    lambda x: apply_regex_patterns(x, genre_patterns)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Predicciones de G√©nero Basadas en Expresiones Regulares (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"G√©nero Real: {row['genre']}\")\n",
    "    print(f\"G√©nero Predicho por Regex: {row['predicted_genre_regex']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3.4. Gram√°tica Libre de Contexto (GLC) y An√°lisis Sint√°ctico de Dependencias con SpaCy ---\n",
    "\n",
    "# Definir palabras clave para las reglas sint√°cticas (lemas, sin tildes por normalizaci√≥n)\n",
    "genre_keywords_lemmas = {\n",
    "    \"comedia\": [\"divertir\", \"gracioso\", \"entretener\", \"absurdo\", \"ingenioso\", \"reir\", \"humor\", \"carcajad\"],\n",
    "    \"drama\": [\"profundo\", \"emotivo\", \"triste\", \"reflexivo\", \"real\", \"conmovedor\", \"llorar\", \"impactar\"],\n",
    "    \"thriller\": [\"intenso\", \"suspenso\", \"giro\", \"inesperado\", \"misterio\", \"intriga\"],\n",
    "    \"ciencia ficcion\": [\"efecto\", \"especial\", \"ciencia\", \"ficcion\", \"futurista\", \"espacio\", \"tecnologia\"]\n",
    "}\n",
    "\n",
    "def analyze_genre_patterns_spacy(doc, genre_keywords_lemmas):\n",
    "    \"\"\"\n",
    "    Analiza un documento SpaCy para detectar patrones sint√°cticos que sugieran un g√©nero.\n",
    "    \"\"\"\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc): # Manejar posibles valores no-Doc (ej. NaNs)\n",
    "        return [(\"N/A\", 0)]\n",
    "\n",
    "    detected_genres = Counter()\n",
    "\n",
    "    for sentence in doc.sents: # Iterar por cada oraci√≥n del documento\n",
    "        for token in sentence:\n",
    "            # Regla 1: Adjetivo que describe 'pel√≠cula' o 'historia'\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                # Si es un atributo de 'ser' (ej. \"pel√≠cula es [adjetivo]\")\n",
    "                if token.head.lemma_ == \"ser\": # y el token modifica al verbo \"ser\"\n",
    "                    for child_of_head in token.head.children:\n",
    "                        # nsubj: nominal subject\n",
    "                        if child_of_head.dep_ == \"nsubj\" and child_of_head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                            for genre, keywords in genre_keywords_lemmas.items():\n",
    "                                if token.lemma_ in keywords:\n",
    "                                    detected_genres[genre] += 1\n",
    "\n",
    "                # Si el adjetivo modifica directamente un sustantivo (ej. \"divertida pel√≠cula\")\n",
    "                if token.dep_ == \"amod\" and token.head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                    for genre, keywords in genre_keywords_lemmas.items():\n",
    "                        if token.lemma_ in keywords:\n",
    "                            detected_genres[genre] += 1\n",
    "\n",
    "            # Regla 2: Verbos de sentimiento/reacci√≥n\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords: # ej. \"re√≠r\" para comedia, \"llorar\" para drama\n",
    "                        detected_genres[genre] += 1\n",
    "\n",
    "            # Regla 3: Sustantivos clave (ej. 'humor', 'suspenso', 'drama')\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 0.5 # Menor peso, ya que pueden ser m√°s ambiguos\n",
    "\n",
    "            # Regla 4: Combinaciones de sustantivo + adjetivo clave (sin depender de un verbo central como 'ser')\n",
    "            # Ej: 'humor absurdo' -> comedia, 'drama familiar' -> drama\n",
    "            if token.pos_ == \"ADJ\" and token.head.pos_ == \"NOUN\": # adjetivo modificando un sustantivo\n",
    "                if token.head.lemma_ == \"humor\" and token.lemma_ in [\"absurdo\", \"ingenioso\"]:\n",
    "                    detected_genres[\"comedia\"] += 1\n",
    "                if token.head.lemma_ == \"drama\" and token.lemma_ in [\"familiar\", \"historico\", \"psicologico\"]:\n",
    "                    detected_genres[\"drama\"] += 1\n",
    "                if token.head.lemma_ == \"efecto\" and token.lemma_ == \"especial\":\n",
    "                    detected_genres[\"ciencia ficcion\"] += 1\n",
    "                if token.head.lemma_ == \"giro\" and token.lemma_ == \"inesperado\":\n",
    "                    detected_genres[\"thriller\"] += 1\n",
    "\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1) # Devuelve el g√©nero m√°s detectado\n",
    "\n",
    "df['predicted_genre_spacy_rules'] = df['spacy_doc'].apply(\n",
    "    lambda doc: analyze_genre_patterns_spacy(doc, genre_keywords_lemmas)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Predicciones de G√©nero Basadas en Reglas Sint√°cticas (SpaCy) (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"G√©nero Real: {row['genre']}\")\n",
    "    print(f\"G√©nero Predicho por Reglas SpaCy: {row['predicted_genre_spacy_rules']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Opcional: Visualizar un √°rbol de dependencias para entender mejor (solo si tienes la UI adecuada)\n",
    "# from spacy import displacy\n",
    "# print(\"\\n--- Visualizaci√≥n de un √Årbol de Dependencias (ejemplo) ---\")\n",
    "# # Aseg√∫rate de que df['spacy_doc'][0] sea un objeto Doc v√°lido (no NaN)\n",
    "# if isinstance(df['spacy_doc'][0], spacy.tokens.doc.Doc):\n",
    "#     displacy.render(df['spacy_doc'][0], style=\"dep\", jupyter=True, options={\"distance\": 90})\n",
    "# else:\n",
    "#     print(\"El primer documento de SpaCy no es v√°lido para visualizaci√≥n.\")\n",
    "\n",
    "print(\"\\n--- Proceso Completado ---\")\n",
    "# Puedes guardar el DataFrame con las nuevas columnas si lo deseas\n",
    "# df.to_excel(\"reviews_processed.xlsx\", index=False)\n",
    "# df.to_csv(\"reviews_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192db0b7",
   "metadata": {},
   "source": [
    "- ‚úî Limpieza de texto (sin eliminar stopwords)\n",
    "- ‚úî Creaci√≥n de columnas limpias\n",
    "- ‚úî Estad√≠stica descriptiva por g√©nero\n",
    "- ‚úî An√°lisis con expresiones regulares\n",
    "- ‚úî An√°lisis sint√°ctico + reglas de GLC con SpaCy\n",
    "\n",
    "üß© C√ìDIGO FINAL COMPLETO ‚Äî Limpieza + Estad√≠stica Sint√°ctica + GLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed418f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.stats import fisher_exact\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Angelica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Original (primeras 5 filas) ---\n",
      "               film_name   gender  film_avg_rate  review_rate  \\\n",
      "0  Ocho apellidos vascos  Comedia            6.0          3.0   \n",
      "1  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "2  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "3  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "4  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "\n",
      "                                        review_title  \\\n",
      "0     OCHO APELLIDOS VASCOS...Y NING√öN NOMBRE PROPIO   \n",
      "1                                     El perro verde   \n",
      "2  Si no eres de comer mierda... no te comas esta...   \n",
      "3                                    Aida: The movie   \n",
      "4               UN HOMBRE SOLO (Julio Iglesias 1987)   \n",
      "\n",
      "                                         review_text    genre  \n",
      "0  La mayor virtud de esta pel√≠cula es su existen...  comedia  \n",
      "1  No soy un experto cin√©filo, pero pocas veces m...  comedia  \n",
      "2  Si no eres un incondicional del humor estilo T...  comedia  \n",
      "3  No s√© qu√© est√° pasando, si la gente se deja ll...  comedia  \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...  comedia   \n",
      "\n",
      "\n",
      "--- Ejemplo de limpieza general ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta pel√≠cula es su existen...   \n",
      "1  No soy un experto cin√©filo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No s√© qu√© est√° pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                              cleaned_review_general  \n",
      "0  la mayor virtud de esta pelicula es su existen...  \n",
      "1  no soy un experto cinefilo, pero pocas veces m...  \n",
      "2  si no eres un incondicional del humor estilo t...  \n",
      "3  no se que esta pasando, si la gente se deja ll...  \n",
      "4  pero cuando amanece,y me quedo solo,siento en ...   \n",
      "\n",
      "\n",
      "--- Ejemplo de texto lematizado ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta pel√≠cula es su existen...   \n",
      "1  No soy un experto cin√©filo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No s√© qu√© est√° pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                         cleaned_review_spacy_lemmas  \n",
      "0  el mayor virtud de este pelicula ser su hecho ...  \n",
      "1  no ser uno experto cinefilo , pero poco vez yo...  \n",
      "2  si no ser uno incondicional del humor estilo t...  \n",
      "3  no √©l que este pasar , si el gente √©l dejar ll...  \n",
      "4  pero cuando amanecer , y yo quedo solo , senti...   \n",
      "\n",
      "--- INICIANDO AN√ÅLISIS ESTAD√çSTICO ---\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Angelica/nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Angelica\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 3.1 Conteo y estad√≠sticas b√°sicas\u001b[39;00m\n\u001b[32m     89\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_spacy_lemmas\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x).split()))\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mREVIEW_TEXT_COLUMN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspanish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Estad√≠sticas de longitud ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[REVIEW_TEXT_COLUMN, \u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m]].head(), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 3.1 Conteo y estad√≠sticas b√°sicas\u001b[39;00m\n\u001b[32m     89\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_spacy_lemmas\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x).split()))\n\u001b[32m     90\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m] = df[REVIEW_TEXT_COLUMN].apply(\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspanish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     92\u001b[39m )\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Estad√≠sticas de longitud ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[REVIEW_TEXT_COLUMN, \u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m]].head(), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Angelica/nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Angelica\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# --- PARTE 0: CONFIGURACI√ìN INICIAL ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Descargar el modelo SpaCy espa√±ol si no est√° disponible\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# --- PARTE 1: CARGA DEL DATASET ---\n",
    "file_path = \"data/reviews_filmaffinity_limpio.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "GENRE_COLUMN = 'gender'\n",
    "REVIEW_TEXT_COLUMN = 'review_text'\n",
    "\n",
    "df['genre'] = df[GENRE_COLUMN].apply(lambda x: str(x).split()[-1].lower())\n",
    "\n",
    "print(\"--- DataFrame Original (primeras 5 filas) ---\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# --- PARTE 2: LIMPIEZA DE TEXTO (sin eliminar stopwords) ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_general(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('√É¬°','√°').replace('√É¬©','√©').replace('√É¬≠','√≠').replace('√É¬≥','√≥').replace('√É¬∫','√∫')\n",
    "    text = text.replace('√É¬±','√±').replace('√¢‚Ç¨‚Ñ¢',\"'\").replace('√¢‚Ç¨≈ì','\"').replace('√¢‚Ç¨','\"')\n",
    "    text = text.replace('√°','a').replace('√©','e').replace('√≠','i').replace('√≥','o').replace('√∫','u').replace('√±','n')\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±\\s.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha or token.text in [\",\", \".\", \"!\", \"?\"]]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Aplicar limpieza\n",
    "df['cleaned_review_general'] = df[REVIEW_TEXT_COLUMN].apply(clean_text_general)\n",
    "\n",
    "print(\"\\n--- Ejemplo de limpieza general ---\")\n",
    "print(df[['review_text', 'cleaned_review_general']].head(), \"\\n\")\n",
    "\n",
    "# Procesar con SpaCy\n",
    "docs = nlp.pipe(df['cleaned_review_general'].astype(str), batch_size=500, n_process=-1)\n",
    "df['spacy_doc'] = list(docs)\n",
    "\n",
    "df['cleaned_review_spacy_lemmas'] = df['spacy_doc'].apply(\n",
    "    lambda doc: ' '.join([token.lemma_ for token in doc if token.is_alpha or token.text in [\",\", \".\", \"!\", \"?\"]])\n",
    ")\n",
    "\n",
    "print(\"\\n--- Ejemplo de texto lematizado ---\")\n",
    "print(df[['review_text', 'cleaned_review_spacy_lemmas']].head(), \"\\n\")\n",
    "\n",
    "# --- PARTE 3: ESTAD√çSTICA SINT√ÅCTICA + GLC ---\n",
    "print(\"--- INICIANDO AN√ÅLISIS ESTAD√çSTICO ---\")\n",
    "\n",
    "# 3.1 Conteo y estad√≠sticas b√°sicas\n",
    "df['num_palabras_limpias'] = df['cleaned_review_spacy_lemmas'].apply(lambda x: len(str(x).split()))\n",
    "df['num_oraciones'] = df[REVIEW_TEXT_COLUMN].apply(\n",
    "    lambda x: len(nltk.sent_tokenize(str(x), language='spanish')) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Estad√≠sticas de longitud ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'num_palabras_limpias', 'num_oraciones']].head(), \"\\n\")\n",
    "\n",
    "print(\"\\n--- Estad√≠sticas descriptivas por g√©nero ---\")\n",
    "print(df.groupby('genre')['num_palabras_limpias'].describe(), \"\\n\")\n",
    "print(df.groupby('genre')['num_oraciones'].describe(), \"\\n\")\n",
    "\n",
    "# 3.2 Frecuencia de palabras y n-gramas\n",
    "def get_top_n_items(corpus, n_items=None, item_type='word', n_gram_size=1):\n",
    "    vec = Counter()\n",
    "    for review in corpus:\n",
    "        tokens = str(review).split()\n",
    "        if item_type == 'word':\n",
    "            for word in tokens:\n",
    "                vec[word] += 1\n",
    "        elif item_type == 'ngram':\n",
    "            if len(tokens) >= n_gram_size:\n",
    "                for ngram_item in ngrams(tokens, n_gram_size):\n",
    "                    vec[' '.join(ngram_item)] += 1\n",
    "    return vec.most_common(n_items)\n",
    "\n",
    "print(\"\\n--- Frecuencia de Palabras y N-gramas ---\")\n",
    "for genre_val in df['genre'].unique():\n",
    "    genre_corpus = df[df['genre'] == genre_val]['cleaned_review_spacy_lemmas']\n",
    "    print(f\"\\n***** G√©nero: {genre_val.upper()} *****\")\n",
    "    print(\"Top 10 palabras:\", get_top_n_items(genre_corpus, 10))\n",
    "    print(\"Top 5 bigramas:\", get_top_n_items(genre_corpus, 5, 'ngram', 2))\n",
    "    print(\"Top 3 trigramas:\", get_top_n_items(genre_corpus, 3, 'ngram', 3))\n",
    "\n",
    "# 3.3 An√°lisis con expresiones regulares\n",
    "genre_patterns = {\n",
    "    'comedia': [\n",
    "        r'pelicula\\s+(?:muy\\s+)?(?:divertida|graciosa|entretenida|absurda)',\n",
    "        r'(?:reir|carcajad|humor\\s+(?:excelente|buen|genial))'\n",
    "    ],\n",
    "    'drama': [\n",
    "        r'historia\\s+(?:profunda|emotiva|triste|reflexiva|real)',\n",
    "        r'(?:llorar|conmovedor|impacto\\s+emocional)'\n",
    "    ],\n",
    "    'thriller': [r'(?:intenso|suspenso|giro\\s+inesperado|misterio|intriga)'],\n",
    "    'ciencia ficcion': [r'(?:efecto\\s+especial|ciencia\\s+ficcion|futurista|espacio|tecnologia)']\n",
    "}\n",
    "\n",
    "def apply_regex_patterns(text, patterns):\n",
    "    if not isinstance(text, str):\n",
    "        return [(\"N/A\", 0)]\n",
    "    detected_genres = Counter()\n",
    "    for genre, regex_list in patterns.items():\n",
    "        for pattern in regex_list:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                detected_genres[genre] += 1\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1)\n",
    "\n",
    "df['predicted_genre_regex'] = df['cleaned_review_general'].apply(lambda x: apply_regex_patterns(x, genre_patterns))\n",
    "\n",
    "print(\"\\n--- Predicciones Regex (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"G√©nero real: {row['genre']}\")\n",
    "    print(f\"G√©nero regex: {row['predicted_genre_regex']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 3.4 Gram√°tica libre de contexto (GLC)\n",
    "genre_keywords_lemmas = {\n",
    "    \"comedia\": [\"divertir\", \"gracioso\", \"entretener\", \"absurdo\", \"ingenioso\", \"reir\", \"humor\", \"carcajad\"],\n",
    "    \"drama\": [\"profundo\", \"emotivo\", \"triste\", \"reflexivo\", \"real\", \"conmovedor\", \"llorar\", \"impactar\"],\n",
    "    \"thriller\": [\"intenso\", \"suspenso\", \"giro\", \"inesperado\", \"misterio\", \"intriga\"],\n",
    "    \"ciencia ficcion\": [\"efecto\", \"especial\", \"ciencia\", \"ficcion\", \"futurista\", \"espacio\", \"tecnologia\"]\n",
    "}\n",
    "\n",
    "def analyze_genre_patterns_spacy(doc, genre_keywords_lemmas):\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc):\n",
    "        return [(\"N/A\", 0)]\n",
    "\n",
    "    detected_genres = Counter()\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                if token.head.lemma_ == \"ser\":\n",
    "                    for child in token.head.children:\n",
    "                        if child.dep_ == \"nsubj\" and child.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                            for genre, keywords in genre_keywords_lemmas.items():\n",
    "                                if token.lemma_ in keywords:\n",
    "                                    detected_genres[genre] += 1\n",
    "                if token.dep_ == \"amod\" and token.head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                    for genre, keywords in genre_keywords_lemmas.items():\n",
    "                        if token.lemma_ in keywords:\n",
    "                            detected_genres[genre] += 1\n",
    "\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 1\n",
    "\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 0.5\n",
    "\n",
    "            if token.pos_ == \"ADJ\" and token.head.pos_ == \"NOUN\":\n",
    "                if token.head.lemma_ == \"humor\" and token.lemma_ in [\"absurdo\", \"ingenioso\"]:\n",
    "                    detected_genres[\"comedia\"] += 1\n",
    "                if token.head.lemma_ == \"drama\" and token.lemma_ in [\"familiar\", \"historico\", \"psicologico\"]:\n",
    "                    detected_genres[\"drama\"] += 1\n",
    "                if token.head.lemma_ == \"efecto\" and token.lemma_ == \"especial\":\n",
    "                    detected_genres[\"ciencia ficcion\"] += 1\n",
    "                if token.head.lemma_ == \"giro\" and token.lemma_ == \"inesperado\":\n",
    "                    detected_genres[\"thriller\"] += 1\n",
    "\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1)\n",
    "\n",
    "df['predicted_genre_spacy_rules'] = df['spacy_doc'].apply(lambda doc: analyze_genre_patterns_spacy(doc, genre_keywords_lemmas))\n",
    "\n",
    "print(\"\\n--- Predicciones SpaCy GLC (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"G√©nero real: {row['genre']}\")\n",
    "    print(f\"G√©nero SpaCy: {row['predicted_genre_spacy_rules']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- PROCESO COMPLETADO ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
