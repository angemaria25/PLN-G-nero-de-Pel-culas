{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5300f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Angelica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Original (primeras 5 filas) ---\n",
      "               film_name   gender  film_avg_rate  review_rate  \\\n",
      "0  Ocho apellidos vascos  Comedia            6.0          3.0   \n",
      "1  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "2  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "3  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "4  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "\n",
      "                                        review_title  \\\n",
      "0     OCHO APELLIDOS VASCOS...Y NINGÚN NOMBRE PROPIO   \n",
      "1                                     El perro verde   \n",
      "2  Si no eres de comer mierda... no te comas esta...   \n",
      "3                                    Aida: The movie   \n",
      "4               UN HOMBRE SOLO (Julio Iglesias 1987)   \n",
      "\n",
      "                                         review_text    genre  \n",
      "0  La mayor virtud de esta película es su existen...  comedia  \n",
      "1  No soy un experto cinéfilo, pero pocas veces m...  comedia  \n",
      "2  Si no eres un incondicional del humor estilo T...  comedia  \n",
      "3  No sé qué está pasando, si la gente se deja ll...  comedia  \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...  comedia  \n",
      "\n",
      "Número total de comentarios cargados: 8603\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- INICIANDO LIMPIEZA DE DATOS ---\n",
      "\n",
      "--- Comentarios después de limpieza general (primeras 5 filas) ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta película es su existen...   \n",
      "1  No soy un experto cinéfilo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No sé qué está pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                              cleaned_review_general  \n",
      "0  la mayor virtud de esta pelicula es su existen...  \n",
      "1  no soy un experto cinefilo pero pocas veces me...  \n",
      "2  si no eres un incondicional del humor estilo t...  \n",
      "3  no se que esta pasando si la gente se deja lle...  \n",
      "4  pero cuando amanecey me quedo solosiento en el...  \n",
      "\n",
      "Procesando comentarios con SpaCy para lematización y eliminación de stopwords...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import spacy\n",
    "# from spacy import displacy # Descomenta si quieres usarlo para visualizar árboles\n",
    "\n",
    "# --- 0. Configuración Inicial y Descarga de Recursos ---\n",
    "# Descargar recursos de NLTK (solo la primera vez que se ejecute en un entorno nuevo)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Descargar el modelo de español de SpaCy (solo la primera vez)\n",
    "# Asegúrate de haber instalado spacy previamente: pip install spacy\n",
    "# y luego, en tu terminal: python -m spacy download es_core_news_sm\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"Descargando modelo de español de SpaCy (es_core_news_sm)...\")\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# --- 1. Cargar tu Dataset ---\n",
    "# --- ¡¡¡ CONFIGURA ESTAS DOS LÍNEAS SEGÚN TU ARCHIVO !!! ---\n",
    "file_path = \"data/reviews_filmaffinity_limpio.csv\" # <--- ¡CAMBIA ESTO A LA RUTA Y NOMBRE DE TU ARCHIVO!\n",
    "file_type = \"csv\" # <--- ¡CAMBIA ESTO A \"csv\" SI TU ARCHIVO ES CSV!\n",
    "\n",
    "if file_type == \"csv\":\n",
    "    df = pd.read_csv(file_path)\n",
    "elif file_type == \"excel\":\n",
    "    df = pd.read_excel(file_path)\n",
    "else:\n",
    "    raise ValueError(\"Tipo de archivo no soportado. Usa 'csv' o 'excel'.\")\n",
    "\n",
    "# --- ¡¡¡ VERIFICA ESTOS NOMBRES DE COLUMNA EN TU DATASET !!! ---\n",
    "# Ajusta si tus columnas tienen nombres diferentes en tu archivo\n",
    "GENRE_COLUMN = 'gender'\n",
    "REVIEW_TEXT_COLUMN = 'review_text'\n",
    "\n",
    "# Renombrar columnas para consistencia si no tienen los nombres esperados\n",
    "if GENRE_COLUMN not in df.columns:\n",
    "    print(f\"ERROR: La columna de género '{GENRE_COLUMN}' no se encontró. Por favor, ajusta 'GENRE_COLUMN' o renombra tu columna en el archivo.\")\n",
    "    exit()\n",
    "if REVIEW_TEXT_COLUMN not in df.columns:\n",
    "    print(f\"ERROR: La columna de texto de reseña '{REVIEW_TEXT_COLUMN}' no se encontró. Por favor, ajusta 'REVIEW_TEXT_COLUMN' o renombra tu columna en el archivo.\")\n",
    "    exit()\n",
    "\n",
    "# Asegurarse de que los géneros sean limpios para la clasificación\n",
    "# Por ejemplo, de 'Ocho apellido comedia' -> 'comedia'\n",
    "# Esto asume que el género relevante es la última palabra de la cadena en 'film_genre'\n",
    "df['genre'] = df[GENRE_COLUMN].apply(lambda x: str(x).split()[-1].lower()) # Asegura que x sea string\n",
    "\n",
    "print(\"--- DataFrame Original (primeras 5 filas) ---\")\n",
    "print(df.head())\n",
    "print(f\"\\nNúmero total de comentarios cargados: {len(df)}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- PARTE 2: LIMPIEZA DE DATOS (PREPROCESAMIENTO) ---\n",
    "print(\"--- INICIANDO LIMPIEZA DE DATOS ---\")\n",
    "\n",
    "# 2.1 Función de Preprocesamiento Integral\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words_spanish = set(stopwords.words('spanish'))\n",
    "\n",
    "def clean_text_general(text):\n",
    "    \"\"\"\n",
    "    Realiza una limpieza general del texto:\n",
    "    - Convierte a minúsculas.\n",
    "    - Decodifica caracteres especiales (tildes, eñes mal codificadas).\n",
    "    - Elimina URLs, puntuación y números.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): # Asegura que la entrada sea string\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower() # Convierte a minúsculas\n",
    "\n",
    "    # Decodificación de caracteres comunes en español que pueden venir mal codificados\n",
    "    text = text.replace('Ã¡', 'á').replace('Ã©', 'é').replace('Ã­', 'í').replace('Ã³', 'ó').replace('Ãº', 'ú')\n",
    "    text = text.replace('Ã±', 'ñ').replace('â€™', \"'\").replace('â€œ', '\"').replace('â€ ', '\"')\n",
    "    text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u') # Normalización sin tildes para evitar problemas con algunos tokenizadores o modelos que no las manejen bien\n",
    "    text = text.replace('ñ', 'n')\n",
    "\n",
    "\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Eliminar puntuación y caracteres especiales (solo deja letras y espacios)\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s]', '', text) # Deja letras (incluyendo acentuadas y ñ) y espacios\n",
    "\n",
    "    # Eliminar números (ya los elimina la regex anterior, pero por si acaso)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Eliminar múltiples espacios en blanco\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    \"\"\"\n",
    "    Procesa texto con SpaCy para obtener lemas limpios (sin stopwords y no alfanuméricos).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    # is_alpha: filtra puntuación y números\n",
    "    # is_stop: filtra stopwords\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Aplicar la limpieza general a la columna de comentarios\n",
    "df['cleaned_review_general'] = df[REVIEW_TEXT_COLUMN].apply(clean_text_general)\n",
    "print(\"\\n--- Comentarios después de limpieza general (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'cleaned_review_general']].head())\n",
    "\n",
    "\n",
    "# Aplicar SpaCy para lematización y eliminación de stopwords\n",
    "# Usaremos nlp.pipe para mayor eficiencia si el dataset es grande\n",
    "print(\"\\nProcesando comentarios con SpaCy para lematización y eliminación de stopwords...\")\n",
    "# Esto es más eficiente que .apply(lambda x: spacy_preprocess(x)) para datasets grandes\n",
    "docs = nlp.pipe(df['cleaned_review_general'].astype(str), batch_size=500, n_process=-1) # n_process=-1 usa todos los núcleos disponibles\n",
    "df['spacy_doc'] = list(docs) # Almacena los objetos Doc de SpaCy\n",
    "\n",
    "# Extraer lemas limpios de los objetos Doc\n",
    "df['cleaned_review_spacy_lemmas'] = df['spacy_doc'].apply(\n",
    "    lambda doc: ' '.join([token.lemma_ for token in doc if token.is_alpha and not token.is_stop])\n",
    ")\n",
    "\n",
    "print(\"\\n--- Comentarios después de SpaCy (lemas limpios) (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'cleaned_review_spacy_lemmas']].head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f637fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARTE 3: ESTADÍSTICA A PARTIR DE LOS COMENTARIOS ---\n",
    "print(\"--- INICIANDO ANÁLISIS ESTADÍSTICO ---\")\n",
    "\n",
    "# --- 3.1. Conteo y Estadísticas Básicas ---\n",
    "\n",
    "# Número de palabras por comentario (después de limpieza)\n",
    "df['num_palabras_limpias'] = df['cleaned_review_spacy_lemmas'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Número de oraciones por comentario (antes de la limpieza profunda, para mantener la estructura original)\n",
    "df['num_oraciones'] = df[REVIEW_TEXT_COLUMN].apply(\n",
    "    lambda x: len(nltk.sent_tokenize(str(x), language='spanish')) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Estadísticas de longitud de comentarios (primeras 5 filas) ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'num_palabras_limpias', 'num_oraciones']].head())\n",
    "\n",
    "# Estadísticas descriptivas de longitud por género\n",
    "print(\"\\n--- Estadísticas descriptivas de Número de Palabras Limpias por Género ---\")\n",
    "print(df.groupby('genre')['num_palabras_limpias'].describe())\n",
    "\n",
    "print(\"\\n--- Estadísticas descriptivas de Número de Oraciones por Género ---\")\n",
    "print(df.groupby('genre')['num_oraciones'].describe())\n",
    "\n",
    "\n",
    "# --- 3.2. Frecuencia de Palabras y N-gramas por Género ---\n",
    "\n",
    "def get_top_n_items(corpus, n_items=None, item_type='word', n_gram_size=1):\n",
    "    \"\"\"\n",
    "    Obtiene los N ítems más frecuentes (palabras o n-gramas) de un corpus.\n",
    "    item_type: 'word' para palabras, 'ngram' para n-gramas.\n",
    "    n_gram_size: tamaño del n-grama si item_type es 'ngram'.\n",
    "    \"\"\"\n",
    "    vec = Counter()\n",
    "    for review in corpus:\n",
    "        tokens = str(review).split() # Asegura que review sea string\n",
    "        if item_type == 'word':\n",
    "            for word in tokens:\n",
    "                vec[word] += 1\n",
    "        elif item_type == 'ngram':\n",
    "            if len(tokens) >= n_gram_size: # Asegura que hay suficientes tokens para el n-grama\n",
    "                for ngram_item in ngrams(tokens, n_gram_size):\n",
    "                    vec[' '.join(ngram_item)] += 1\n",
    "    return vec.most_common(n_items)\n",
    "\n",
    "print(\"\\n--- Frecuencia de Palabras y N-gramas por Género ---\")\n",
    "all_genres = df['genre'].unique()\n",
    "\n",
    "for genre_val in all_genres:\n",
    "    genre_corpus = df[df['genre'] == genre_val]['cleaned_review_spacy_lemmas']\n",
    "\n",
    "    print(f\"\\n***** Análisis para el género: '{genre_val.upper()}' *****\")\n",
    "\n",
    "    # Palabras más frecuentes\n",
    "    top_words = get_top_n_items(genre_corpus, n_items=10, item_type='word')\n",
    "    print(f\"  Top 10 palabras (lemas): {top_words}\")\n",
    "\n",
    "    # Bigramas más frecuentes\n",
    "    top_bigrams = get_top_n_items(genre_corpus, n_items=5, item_type='ngram', n_gram_size=2)\n",
    "    print(f\"  Top 5 bigramas (lemas): {top_bigrams}\")\n",
    "\n",
    "    # Trigramas más frecuentes\n",
    "    top_trigrams = get_top_n_items(genre_corpus, n_items=3, item_type='ngram', n_gram_size=3)\n",
    "    print(f\"  Top 3 trigramas (lemas): {top_trigrams}\")\n",
    "\n",
    "\n",
    "# --- 3.3. Análisis con Expresiones Regulares ---\n",
    "\n",
    "# Define patrones específicos para cada género. Estos son solo ejemplos, puedes expandirlos.\n",
    "genre_patterns = {\n",
    "    'comedia': [\n",
    "        r'pelicula\\s+(?:muy\\s+)?(?:divertida|graciosa|entretenida|absurda)', # Usamos ?: para grupos no capturadores\n",
    "        r'(?:reir|carcajad|humor\\s+(?:excelente|buen|genial))'\n",
    "    ],\n",
    "    'drama': [\n",
    "        r'historia\\s+(?:profunda|emotiva|triste|reflexiva|real)',\n",
    "        r'(?:llorar|conmovedor|impacto\\s+emocional)'\n",
    "    ],\n",
    "    'thriller': [\n",
    "        r'(?:intenso|suspenso|giro\\s+inesperado|misterio|intriga)'\n",
    "    ],\n",
    "    'ciencia ficción': [ # Ajustado a 'ciencia ficcion' por la normalización sin tildes\n",
    "        r'(?:efecto\\s+especial|ciencia\\s+ficcion|futurista|espacio|tecnologia)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def apply_regex_patterns(text, patterns):\n",
    "    \"\"\"\n",
    "    Aplica una lista de patrones regex a un texto y cuenta cuántos coinciden.\n",
    "    Devuelve un contador de los géneros detectados.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return [(\"N/A\", 0)] # Devolver un valor por defecto si no es string\n",
    "\n",
    "    detected_genres = Counter()\n",
    "    for genre, regex_list in patterns.items():\n",
    "        for pattern in regex_list:\n",
    "            if re.search(pattern, text, re.IGNORECASE): # re.IGNORECASE para coincidir independientemente de mayúsculas/minúsculas\n",
    "                detected_genres[genre] += 1\n",
    "    \n",
    "    if not detected_genres: # Si no se detectó ningún género, devuelve 'desconocido'\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1) # Devuelve el género más detectado por esta técnica\n",
    "\n",
    "df['predicted_genre_regex'] = df['cleaned_review_general'].apply(\n",
    "    lambda x: apply_regex_patterns(x, genre_patterns)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Predicciones de Género Basadas en Expresiones Regulares (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"Género Real: {row['genre']}\")\n",
    "    print(f\"Género Predicho por Regex: {row['predicted_genre_regex']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3.4. Gramática Libre de Contexto (GLC) y Análisis Sintáctico de Dependencias con SpaCy ---\n",
    "\n",
    "# Definir palabras clave para las reglas sintácticas (lemas, sin tildes por normalización)\n",
    "genre_keywords_lemmas = {\n",
    "    \"comedia\": [\"divertir\", \"gracioso\", \"entretener\", \"absurdo\", \"ingenioso\", \"reir\", \"humor\", \"carcajad\"],\n",
    "    \"drama\": [\"profundo\", \"emotivo\", \"triste\", \"reflexivo\", \"real\", \"conmovedor\", \"llorar\", \"impactar\"],\n",
    "    \"thriller\": [\"intenso\", \"suspenso\", \"giro\", \"inesperado\", \"misterio\", \"intriga\"],\n",
    "    \"ciencia ficcion\": [\"efecto\", \"especial\", \"ciencia\", \"ficcion\", \"futurista\", \"espacio\", \"tecnologia\"]\n",
    "}\n",
    "\n",
    "def analyze_genre_patterns_spacy(doc, genre_keywords_lemmas):\n",
    "    \"\"\"\n",
    "    Analiza un documento SpaCy para detectar patrones sintácticos que sugieran un género.\n",
    "    \"\"\"\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc): # Manejar posibles valores no-Doc (ej. NaNs)\n",
    "        return [(\"N/A\", 0)]\n",
    "\n",
    "    detected_genres = Counter()\n",
    "\n",
    "    for sentence in doc.sents: # Iterar por cada oración del documento\n",
    "        for token in sentence:\n",
    "            # Regla 1: Adjetivo que describe 'película' o 'historia'\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                # Si es un atributo de 'ser' (ej. \"película es [adjetivo]\")\n",
    "                if token.head.lemma_ == \"ser\": # y el token modifica al verbo \"ser\"\n",
    "                    for child_of_head in token.head.children:\n",
    "                        # nsubj: nominal subject\n",
    "                        if child_of_head.dep_ == \"nsubj\" and child_of_head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                            for genre, keywords in genre_keywords_lemmas.items():\n",
    "                                if token.lemma_ in keywords:\n",
    "                                    detected_genres[genre] += 1\n",
    "\n",
    "                # Si el adjetivo modifica directamente un sustantivo (ej. \"divertida película\")\n",
    "                if token.dep_ == \"amod\" and token.head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                    for genre, keywords in genre_keywords_lemmas.items():\n",
    "                        if token.lemma_ in keywords:\n",
    "                            detected_genres[genre] += 1\n",
    "\n",
    "            # Regla 2: Verbos de sentimiento/reacción\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords: # ej. \"reír\" para comedia, \"llorar\" para drama\n",
    "                        detected_genres[genre] += 1\n",
    "\n",
    "            # Regla 3: Sustantivos clave (ej. 'humor', 'suspenso', 'drama')\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 0.5 # Menor peso, ya que pueden ser más ambiguos\n",
    "\n",
    "            # Regla 4: Combinaciones de sustantivo + adjetivo clave (sin depender de un verbo central como 'ser')\n",
    "            # Ej: 'humor absurdo' -> comedia, 'drama familiar' -> drama\n",
    "            if token.pos_ == \"ADJ\" and token.head.pos_ == \"NOUN\": # adjetivo modificando un sustantivo\n",
    "                if token.head.lemma_ == \"humor\" and token.lemma_ in [\"absurdo\", \"ingenioso\"]:\n",
    "                    detected_genres[\"comedia\"] += 1\n",
    "                if token.head.lemma_ == \"drama\" and token.lemma_ in [\"familiar\", \"historico\", \"psicologico\"]:\n",
    "                    detected_genres[\"drama\"] += 1\n",
    "                if token.head.lemma_ == \"efecto\" and token.lemma_ == \"especial\":\n",
    "                    detected_genres[\"ciencia ficcion\"] += 1\n",
    "                if token.head.lemma_ == \"giro\" and token.lemma_ == \"inesperado\":\n",
    "                    detected_genres[\"thriller\"] += 1\n",
    "\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1) # Devuelve el género más detectado\n",
    "\n",
    "df['predicted_genre_spacy_rules'] = df['spacy_doc'].apply(\n",
    "    lambda doc: analyze_genre_patterns_spacy(doc, genre_keywords_lemmas)\n",
    ")\n",
    "\n",
    "print(\"\\n--- Predicciones de Género Basadas en Reglas Sintácticas (SpaCy) (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"Género Real: {row['genre']}\")\n",
    "    print(f\"Género Predicho por Reglas SpaCy: {row['predicted_genre_spacy_rules']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Opcional: Visualizar un árbol de dependencias para entender mejor (solo si tienes la UI adecuada)\n",
    "# from spacy import displacy\n",
    "# print(\"\\n--- Visualización de un Árbol de Dependencias (ejemplo) ---\")\n",
    "# # Asegúrate de que df['spacy_doc'][0] sea un objeto Doc válido (no NaN)\n",
    "# if isinstance(df['spacy_doc'][0], spacy.tokens.doc.Doc):\n",
    "#     displacy.render(df['spacy_doc'][0], style=\"dep\", jupyter=True, options={\"distance\": 90})\n",
    "# else:\n",
    "#     print(\"El primer documento de SpaCy no es válido para visualización.\")\n",
    "\n",
    "print(\"\\n--- Proceso Completado ---\")\n",
    "# Puedes guardar el DataFrame con las nuevas columnas si lo deseas\n",
    "# df.to_excel(\"reviews_processed.xlsx\", index=False)\n",
    "# df.to_csv(\"reviews_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192db0b7",
   "metadata": {},
   "source": [
    "- ✔ Limpieza de texto (sin eliminar stopwords)\n",
    "- ✔ Creación de columnas limpias\n",
    "- ✔ Estadística descriptiva por género\n",
    "- ✔ Análisis con expresiones regulares\n",
    "- ✔ Análisis sintáctico + reglas de GLC con SpaCy\n",
    "\n",
    "🧩 CÓDIGO FINAL COMPLETO — Limpieza + Estadística Sintáctica + GLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed418f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\metrics\\association.py:26: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.stats import fisher_exact\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Angelica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Original (primeras 5 filas) ---\n",
      "               film_name   gender  film_avg_rate  review_rate  \\\n",
      "0  Ocho apellidos vascos  Comedia            6.0          3.0   \n",
      "1  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "2  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "3  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "4  Ocho apellidos vascos  Comedia            6.0          2.0   \n",
      "\n",
      "                                        review_title  \\\n",
      "0     OCHO APELLIDOS VASCOS...Y NINGÚN NOMBRE PROPIO   \n",
      "1                                     El perro verde   \n",
      "2  Si no eres de comer mierda... no te comas esta...   \n",
      "3                                    Aida: The movie   \n",
      "4               UN HOMBRE SOLO (Julio Iglesias 1987)   \n",
      "\n",
      "                                         review_text    genre  \n",
      "0  La mayor virtud de esta película es su existen...  comedia  \n",
      "1  No soy un experto cinéfilo, pero pocas veces m...  comedia  \n",
      "2  Si no eres un incondicional del humor estilo T...  comedia  \n",
      "3  No sé qué está pasando, si la gente se deja ll...  comedia  \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...  comedia   \n",
      "\n",
      "\n",
      "--- Ejemplo de limpieza general ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta película es su existen...   \n",
      "1  No soy un experto cinéfilo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No sé qué está pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                              cleaned_review_general  \n",
      "0  la mayor virtud de esta pelicula es su existen...  \n",
      "1  no soy un experto cinefilo, pero pocas veces m...  \n",
      "2  si no eres un incondicional del humor estilo t...  \n",
      "3  no se que esta pasando, si la gente se deja ll...  \n",
      "4  pero cuando amanece,y me quedo solo,siento en ...   \n",
      "\n",
      "\n",
      "--- Ejemplo de texto lematizado ---\n",
      "                                         review_text  \\\n",
      "0  La mayor virtud de esta película es su existen...   \n",
      "1  No soy un experto cinéfilo, pero pocas veces m...   \n",
      "2  Si no eres un incondicional del humor estilo T...   \n",
      "3  No sé qué está pasando, si la gente se deja ll...   \n",
      "4  \"Pero cuando amanece,y me quedo solo,siento en...   \n",
      "\n",
      "                         cleaned_review_spacy_lemmas  \n",
      "0  el mayor virtud de este pelicula ser su hecho ...  \n",
      "1  no ser uno experto cinefilo , pero poco vez yo...  \n",
      "2  si no ser uno incondicional del humor estilo t...  \n",
      "3  no él que este pasar , si el gente él dejar ll...  \n",
      "4  pero cuando amanecer , y yo quedo solo , senti...   \n",
      "\n",
      "--- INICIANDO ANÁLISIS ESTADÍSTICO ---\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Angelica/nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Angelica\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 3.1 Conteo y estadísticas básicas\u001b[39;00m\n\u001b[32m     89\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_spacy_lemmas\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x).split()))\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mREVIEW_TEXT_COLUMN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspanish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Estadísticas de longitud ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[REVIEW_TEXT_COLUMN, \u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m]].head(), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# 3.1 Conteo y estadísticas básicas\u001b[39;00m\n\u001b[32m     89\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mcleaned_review_spacy_lemmas\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(x).split()))\n\u001b[32m     90\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m] = df[REVIEW_TEXT_COLUMN].apply(\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspanish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m pd.notna(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m     92\u001b[39m )\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Estadísticas de longitud ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[REVIEW_TEXT_COLUMN, \u001b[33m'\u001b[39m\u001b[33mnum_palabras_limpias\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnum_oraciones\u001b[39m\u001b[33m'\u001b[39m]].head(), \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Angelica\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/spanish/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Angelica/nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Angelica\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Angelica\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# --- PARTE 0: CONFIGURACIÓN INICIAL ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Descargar el modelo SpaCy español si no está disponible\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"es_core_news_sm\")\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# --- PARTE 1: CARGA DEL DATASET ---\n",
    "file_path = \"data/reviews_filmaffinity_limpio.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "GENRE_COLUMN = 'gender'\n",
    "REVIEW_TEXT_COLUMN = 'review_text'\n",
    "\n",
    "df['genre'] = df[GENRE_COLUMN].apply(lambda x: str(x).split()[-1].lower())\n",
    "\n",
    "print(\"--- DataFrame Original (primeras 5 filas) ---\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# --- PARTE 2: LIMPIEZA DE TEXTO (sin eliminar stopwords) ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_general(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('Ã¡','á').replace('Ã©','é').replace('Ã­','í').replace('Ã³','ó').replace('Ãº','ú')\n",
    "    text = text.replace('Ã±','ñ').replace('â€™',\"'\").replace('â€œ','\"').replace('â€','\"')\n",
    "    text = text.replace('á','a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u').replace('ñ','n')\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'[^a-záéíóúüñ\\s.,!?]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc if token.is_alpha or token.text in [\",\", \".\", \"!\", \"?\"]]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Aplicar limpieza\n",
    "df['cleaned_review_general'] = df[REVIEW_TEXT_COLUMN].apply(clean_text_general)\n",
    "\n",
    "print(\"\\n--- Ejemplo de limpieza general ---\")\n",
    "print(df[['review_text', 'cleaned_review_general']].head(), \"\\n\")\n",
    "\n",
    "# Procesar con SpaCy\n",
    "docs = nlp.pipe(df['cleaned_review_general'].astype(str), batch_size=500, n_process=-1)\n",
    "df['spacy_doc'] = list(docs)\n",
    "\n",
    "df['cleaned_review_spacy_lemmas'] = df['spacy_doc'].apply(\n",
    "    lambda doc: ' '.join([token.lemma_ for token in doc if token.is_alpha or token.text in [\",\", \".\", \"!\", \"?\"]])\n",
    ")\n",
    "\n",
    "print(\"\\n--- Ejemplo de texto lematizado ---\")\n",
    "print(df[['review_text', 'cleaned_review_spacy_lemmas']].head(), \"\\n\")\n",
    "\n",
    "# --- PARTE 3: ESTADÍSTICA SINTÁCTICA + GLC ---\n",
    "print(\"--- INICIANDO ANÁLISIS ESTADÍSTICO ---\")\n",
    "\n",
    "# 3.1 Conteo y estadísticas básicas\n",
    "df['num_palabras_limpias'] = df['cleaned_review_spacy_lemmas'].apply(lambda x: len(str(x).split()))\n",
    "df['num_oraciones'] = df[REVIEW_TEXT_COLUMN].apply(\n",
    "    lambda x: len(nltk.sent_tokenize(str(x), language='spanish')) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Estadísticas de longitud ---\")\n",
    "print(df[[REVIEW_TEXT_COLUMN, 'num_palabras_limpias', 'num_oraciones']].head(), \"\\n\")\n",
    "\n",
    "print(\"\\n--- Estadísticas descriptivas por género ---\")\n",
    "print(df.groupby('genre')['num_palabras_limpias'].describe(), \"\\n\")\n",
    "print(df.groupby('genre')['num_oraciones'].describe(), \"\\n\")\n",
    "\n",
    "# 3.2 Frecuencia de palabras y n-gramas\n",
    "def get_top_n_items(corpus, n_items=None, item_type='word', n_gram_size=1):\n",
    "    vec = Counter()\n",
    "    for review in corpus:\n",
    "        tokens = str(review).split()\n",
    "        if item_type == 'word':\n",
    "            for word in tokens:\n",
    "                vec[word] += 1\n",
    "        elif item_type == 'ngram':\n",
    "            if len(tokens) >= n_gram_size:\n",
    "                for ngram_item in ngrams(tokens, n_gram_size):\n",
    "                    vec[' '.join(ngram_item)] += 1\n",
    "    return vec.most_common(n_items)\n",
    "\n",
    "print(\"\\n--- Frecuencia de Palabras y N-gramas ---\")\n",
    "for genre_val in df['genre'].unique():\n",
    "    genre_corpus = df[df['genre'] == genre_val]['cleaned_review_spacy_lemmas']\n",
    "    print(f\"\\n***** Género: {genre_val.upper()} *****\")\n",
    "    print(\"Top 10 palabras:\", get_top_n_items(genre_corpus, 10))\n",
    "    print(\"Top 5 bigramas:\", get_top_n_items(genre_corpus, 5, 'ngram', 2))\n",
    "    print(\"Top 3 trigramas:\", get_top_n_items(genre_corpus, 3, 'ngram', 3))\n",
    "\n",
    "# 3.3 Análisis con expresiones regulares\n",
    "genre_patterns = {\n",
    "    'comedia': [\n",
    "        r'pelicula\\s+(?:muy\\s+)?(?:divertida|graciosa|entretenida|absurda)',\n",
    "        r'(?:reir|carcajad|humor\\s+(?:excelente|buen|genial))'\n",
    "    ],\n",
    "    'drama': [\n",
    "        r'historia\\s+(?:profunda|emotiva|triste|reflexiva|real)',\n",
    "        r'(?:llorar|conmovedor|impacto\\s+emocional)'\n",
    "    ],\n",
    "    'thriller': [r'(?:intenso|suspenso|giro\\s+inesperado|misterio|intriga)'],\n",
    "    'ciencia ficcion': [r'(?:efecto\\s+especial|ciencia\\s+ficcion|futurista|espacio|tecnologia)']\n",
    "}\n",
    "\n",
    "def apply_regex_patterns(text, patterns):\n",
    "    if not isinstance(text, str):\n",
    "        return [(\"N/A\", 0)]\n",
    "    detected_genres = Counter()\n",
    "    for genre, regex_list in patterns.items():\n",
    "        for pattern in regex_list:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                detected_genres[genre] += 1\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1)\n",
    "\n",
    "df['predicted_genre_regex'] = df['cleaned_review_general'].apply(lambda x: apply_regex_patterns(x, genre_patterns))\n",
    "\n",
    "print(\"\\n--- Predicciones Regex (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"Género real: {row['genre']}\")\n",
    "    print(f\"Género regex: {row['predicted_genre_regex']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# 3.4 Gramática libre de contexto (GLC)\n",
    "genre_keywords_lemmas = {\n",
    "    \"comedia\": [\"divertir\", \"gracioso\", \"entretener\", \"absurdo\", \"ingenioso\", \"reir\", \"humor\", \"carcajad\"],\n",
    "    \"drama\": [\"profundo\", \"emotivo\", \"triste\", \"reflexivo\", \"real\", \"conmovedor\", \"llorar\", \"impactar\"],\n",
    "    \"thriller\": [\"intenso\", \"suspenso\", \"giro\", \"inesperado\", \"misterio\", \"intriga\"],\n",
    "    \"ciencia ficcion\": [\"efecto\", \"especial\", \"ciencia\", \"ficcion\", \"futurista\", \"espacio\", \"tecnologia\"]\n",
    "}\n",
    "\n",
    "def analyze_genre_patterns_spacy(doc, genre_keywords_lemmas):\n",
    "    if not isinstance(doc, spacy.tokens.doc.Doc):\n",
    "        return [(\"N/A\", 0)]\n",
    "\n",
    "    detected_genres = Counter()\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                if token.head.lemma_ == \"ser\":\n",
    "                    for child in token.head.children:\n",
    "                        if child.dep_ == \"nsubj\" and child.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                            for genre, keywords in genre_keywords_lemmas.items():\n",
    "                                if token.lemma_ in keywords:\n",
    "                                    detected_genres[genre] += 1\n",
    "                if token.dep_ == \"amod\" and token.head.lemma_ in [\"pelicula\", \"historia\", \"trama\", \"film\"]:\n",
    "                    for genre, keywords in genre_keywords_lemmas.items():\n",
    "                        if token.lemma_ in keywords:\n",
    "                            detected_genres[genre] += 1\n",
    "\n",
    "            if token.pos_ == \"VERB\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 1\n",
    "\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                for genre, keywords in genre_keywords_lemmas.items():\n",
    "                    if token.lemma_ in keywords:\n",
    "                        detected_genres[genre] += 0.5\n",
    "\n",
    "            if token.pos_ == \"ADJ\" and token.head.pos_ == \"NOUN\":\n",
    "                if token.head.lemma_ == \"humor\" and token.lemma_ in [\"absurdo\", \"ingenioso\"]:\n",
    "                    detected_genres[\"comedia\"] += 1\n",
    "                if token.head.lemma_ == \"drama\" and token.lemma_ in [\"familiar\", \"historico\", \"psicologico\"]:\n",
    "                    detected_genres[\"drama\"] += 1\n",
    "                if token.head.lemma_ == \"efecto\" and token.lemma_ == \"especial\":\n",
    "                    detected_genres[\"ciencia ficcion\"] += 1\n",
    "                if token.head.lemma_ == \"giro\" and token.lemma_ == \"inesperado\":\n",
    "                    detected_genres[\"thriller\"] += 1\n",
    "\n",
    "    if not detected_genres:\n",
    "        return [(\"desconocido\", 0)]\n",
    "    return detected_genres.most_common(1)\n",
    "\n",
    "df['predicted_genre_spacy_rules'] = df['spacy_doc'].apply(lambda doc: analyze_genre_patterns_spacy(doc, genre_keywords_lemmas))\n",
    "\n",
    "print(\"\\n--- Predicciones SpaCy GLC (primeras 5 filas) ---\")\n",
    "for idx, row in df.head().iterrows():\n",
    "    print(f\"Comentario: '{row[REVIEW_TEXT_COLUMN]}'\")\n",
    "    print(f\"Género real: {row['genre']}\")\n",
    "    print(f\"Género SpaCy: {row['predicted_genre_spacy_rules']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n--- PROCESO COMPLETADO ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
